{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The master_file serves as the glue for all other files in the project.  It is the central file to load training and test data, tag the addresses, standardize the addresses, and compare the different address lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from address_compare import standardizers as stndrdzr\n",
    "from address_compare import comparers as comps\n",
    "from address_compare import matcher as mtch\n",
    "from address_compare.crf_tagger import AddressTagger\n",
    "from address_compare import address_randomizer as add_rndm\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Editable parameters to control the functions\n",
    "retrain_crf_tagger = False #if True, the specified training file will be used to retrain the CRF Tagger\n",
    "standardize_addresses = True #if True, the tagged address components will be standardized (changed to upper case, unit types, street types, etc. changed to long form names)\n",
    "num_rndm_addresses_to_create = 100 #if use_raw_address_files = False, the number of addresses that will be randomly created for use in the tagger and compare functions\n",
    "use_raw_address_files = True #if False, only the specified number of randomly created addresses above will be used\n",
    "group_addresses_intra_list = False #if False, duplicates within a list will not be grouped in order to easily compare against the golden/manual matches\n",
    "\n",
    "view_address_tagger_metrics_against_single_file_only = False #if True, the file will only run the address tagger against the file in file_location_tagger_test_metrics and will calculate the accuracy of the tagger\n",
    "\n",
    "field_name_raw_addresses = 'Single String Address' #represents the name of the field in the raw address files containing the raw address (street information)\n",
    "field_name_record_id = 'Record_ID' #represents the name of the field containing the Record ID in the raw files; if not present in the raw files, populate with None\n",
    "\n",
    "file_location_raw_addresses_1 = 'data\\\\MarijuanaApplicants - test data list 1.xlsx'\n",
    "file_location_raw_addresses_2 = 'data\\\\MarijuanaApplicants - test data list 2.xlsx'\n",
    "\n",
    "file_location_tagger_test_metrics = 'data\\\\standardized tagged washington state addresses.xlsx' #only used if view_address_tagger_metrics.. == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for reading/calling the training data for the CRF Tagger and sending the training data to train the model\n",
    "if retrain_crf_tagger:\n",
    "    with open('data/tagged_addresses.json') as f:\n",
    "        td = json.load(f)\n",
    "    \n",
    "    #send training data to CRF tagger to train the model here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Single File against Tagger and Calculate Accuracy\n",
    "if view_address_tagger_metrics_against_single_file_only:\n",
    "    test_file = pd.read_excel(file_location_tagger_test_metrics, keep_default_na=False, dtype=str)\n",
    "    test_file = stndrdzr.record_id_addition(test_file, field_name_record_id)\n",
    "    tagger = AddressTagger()\n",
    "    tagged_test_file = tagger.series_to_address_df(test_file[field_name_raw_addresses], standardize = standardize_addresses)\n",
    "    crf_tagged_test_file = tagged_test_file.join(test_file['Record_ID'])\n",
    "    \n",
    "    manual_tagged_test_file = test_file[['Record_ID', 'Tagged Street Number','Tagged Pre Street Direction','Tagged Street Name','Tagged Street Type','Tagged Post Street Direction','Tagged Unit Type','Tagged Unit Number']].copy()\n",
    "    manual_tagged_test_file = manual_tagged_test_file.rename(columns={'Tagged Street Number':'STREET_NUMBER',\n",
    "                                                                     'Tagged Pre Street Direction':'PRE_DIRECTION',\n",
    "                                                                     'Tagged Street Name':'STREET_NAME',\n",
    "                                                                     'Tagged Street Type':'STREET_TYPE',\n",
    "                                                                     'Tagged Post Street Direction':'POST_DIRECTION',\n",
    "                                                                     'Tagged Unit Type':'UNIT_TYPE',\n",
    "                                                                     'Tagged Unit Number':'UNIT_NUMBER'})\n",
    "\n",
    "    cols_for_matcher = ['UNIT_TYPE','UNIT_NUMBER','STREET_NUMBER','PRE_DIRECTION','STREET_NAME','STREET_TYPE','POST_DIRECTION']\n",
    "    correctly_tagged_addresses = mtch.exact_matcher(crf_tagged_test_file, manual_tagged_test_file, cols_for_matcher)\n",
    "    \n",
    "    incorrectly_tagged_addresses = crf_tagged_test_file.mask(crf_tagged_test_file.Record_ID.isin(correctly_tagged_addresses['Record_ID_list_1'])).dropna()\n",
    "    total_records = crf_tagged_test_file.shape[0]\n",
    "    correctly_tagged = correctly_tagged_addresses.shape[0]\n",
    "    incorrectly_tagged = incorrectly_tagged_addresses.shape[0]\n",
    "    tagger_accuracy = correctly_tagged / total_records\n",
    "    print ('tagger accuracy = ', tagger_accuracy)\n",
    "    \n",
    "    for col in cols_for_matcher:\n",
    "        precision, recall, fscore, ignore = sklearn.metrics.precision_recall_fscore_support(manual_tagged_test_file[col], crf_tagged_test_file[col], pos_label=None,average='micro')\n",
    "        print ('column = ', col, 'precision = ', precision, 'recall = ', recall, 'f1score = ',fscore)\n",
    "    \n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for reading/calling the 2 lists of raw addresses\n",
    "if use_raw_address_files:\n",
    "    raw_address_list_1 = pd.read_excel(file_location_raw_addresses_1)\n",
    "    raw_address_list_2 = pd.read_excel(file_location_raw_addresses_2)\n",
    "else:\n",
    "    raw_address_list_1 = add_rndm.random_addresses(num_rndm_addresses_to_create, field_name_raw_addresses)\n",
    "    raw_address_list_2 = add_rndm.random_addresses(num_rndm_addresses_to_create, field_name_raw_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a field called Record_ID if it doesn't already exist in the raw address files\n",
    "raw_address_list_1 = stndrdzr.record_id_addition(raw_address_list_1, field_name_record_id)\n",
    "raw_address_list_2 = stndrdzr.record_id_addition(raw_address_list_2, field_name_record_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Empty Missing Columns to Dataframe\n",
    "missing_columns = ['CITY','STATE','ZIP_CODE','UNKNOWN']\n",
    "raw_address_list_1 = stndrdzr.empty_column_addition(raw_address_list_1, missing_columns)\n",
    "raw_address_list_2 = stndrdzr.empty_column_addition(raw_address_list_2, missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate AddressTagger object with default options, which gives the model trained in `Train CRF Model`.ipynb\n",
    "at = AddressTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the trained CRF Tagger on the 2 lists of raw addresses\n",
    "tagged_address_list_1 = at.series_to_address_df(raw_address_list_1[field_name_raw_addresses], standardize = standardize_addresses)\n",
    "tagged_address_list_2 = at.series_to_address_df(raw_address_list_2[field_name_raw_addresses], standardize = standardize_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for Errors in Zip Codes and Replace City Names with Primary City from Zip Code\n",
    "raw_address_list_1 = stndrdzr.fix_cities_zips(raw_address_list_1)\n",
    "raw_address_list_2 = stndrdzr.fix_cities_zips(raw_address_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Remaining Columns from Raw Address Dataframes to Tagged Address Dataframes\n",
    "joined_address_list_1 = tagged_address_list_1.join(raw_address_list_1[['Record_ID','CITY','STATE','ZIP_CODE','UNKNOWN','Zip_Code_Error']])\n",
    "joined_address_list_2 = tagged_address_list_2.join(raw_address_list_2[['Record_ID','CITY','STATE','ZIP_CODE','UNKNOWN','Zip_Code_Error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Addresses with Zip Code Errors (I.e., where the Zip Code is not valid for the given state)\n",
    "error_addresses_list_1 = joined_address_list_1.where(joined_address_list_1.Zip_Code_Error == \"Yes\").dropna()\n",
    "error_addresses_list_2 = joined_address_list_2.where(joined_address_list_2.Zip_Code_Error == \"Yes\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only Addresses without Zip Code Errors (I.e., where the Zip Code is valid for the given state)\n",
    "nonerror_addresses_list_1 = joined_address_list_1.where(joined_address_list_1.Zip_Code_Error == \"No\").dropna()\n",
    "nonerror_addresses_list_2 = joined_address_list_2.where(joined_address_list_2.Zip_Code_Error == \"No\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonerror_addresses_list_1 = nonerror_addresses_list_1.astype({'Record_ID':'int', 'ZIP_CODE':'int'})\n",
    "nonerror_addresses_list_2 = nonerror_addresses_list_2.astype({'Record_ID':'int', 'ZIP_CODE':'int'})\n",
    "nonerror_addresses_list_1 = nonerror_addresses_list_1.astype({'Record_ID':'str', 'ZIP_CODE':'str'})\n",
    "nonerror_addresses_list_2 = nonerror_addresses_list_2.astype({'Record_ID':'str', 'ZIP_CODE':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intra-Grouping of Tagged Address Lists to Consolidate Duplicates\n",
    "if group_addresses_intra_list:\n",
    "    grouped_address_list_1 = stndrdzr.consolidate_address_list(nonerror_addresses_list_1)\n",
    "    grouped_address_list_2 = stndrdzr.consolidate_address_list(nonerror_addresses_list_2)\n",
    "else:\n",
    "    grouped_address_list_1 = nonerror_addresses_list_1.copy()\n",
    "    grouped_address_list_2 = nonerror_addresses_list_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call Either the Exact Match or Learning Match Functions to match the 2 lists\n",
    "exact_matches = mtch.exact_matcher(grouped_address_list_1, grouped_address_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unmatched_address_list_1 = grouped_address_list_1.mask(grouped_address_list_1.Record_ID.isin(exact_matches['Record_ID_list_1'])).dropna()\n",
    "unmatched_address_list_2 = grouped_address_list_2.mask(grouped_address_list_2.Record_ID.isin(exact_matches['Record_ID_list_2'])).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of DataFrames for Excel File\n",
    "dataframes_for_excel = {'raw_addresses_list_1': raw_address_list_1, 'raw_addresses_list2': raw_address_list_2,\n",
    "                        'zip_errors_list1': error_addresses_list_1, 'zip_errors_list2': error_addresses_list_2,\n",
    "                       'exact_matches': exact_matches, 'unmatched_list_1': unmatched_address_list_1,\n",
    "                       'unmatched_list_2': unmatched_address_list_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Dict of DataFrames to Excel\n",
    "writer = pd.ExcelWriter('output\\\\raw_to_matched_addresses.xlsx', engine='xlsxwriter')\n",
    "for sheet, frame in  dataframes_for_excel.items():\n",
    "    frame.to_excel(writer, sheet_name = sheet)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare Output of Exact Match to Manually Tagged Matches\n",
    "if not group_addresses_intra_list:\n",
    "    manual_matches = pd.read_excel('data\\\\marijuana applicants test data - correct matches.xlsx', dtype=str)\n",
    "    golden_exact_matches = manual_matches.where(manual_matches.Match_Type.isin([\"Exact\",\"Standardized Exact\"])).dropna().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Record_ID_list_1 Record_ID_list_2 row_index_list_1 row_index_list_2\n",
      "0                  1                1                0                0\n",
      "1                  3                2                1                1\n",
      "2                  4                3                2                2\n",
      "3                  5                4                3                3\n",
      "4                  6                5                4                4\n",
      "5                  7                6                5                5\n",
      "6                  8                7                6                6\n",
      "7                 11                8                7                7\n",
      "8                 12                9                8                8\n",
      "9                 13               10                9                9\n",
      "10                14               12               10               10\n",
      "11                15               13               11               11\n",
      "12                16               14               12               12\n",
      "13                17               15               13               13\n",
      "14                18               16               14               14\n",
      "15                19               17               15               15\n",
      "16                20               18               16               16\n",
      "17                21               19               17               17\n",
      "18                22               20               18               18\n",
      "19                23               21               19               19\n",
      "20                24               22               20               20\n",
      "21                25               23               21               21\n",
      "22                26               24               22               22\n",
      "23                27               25               23               23\n",
      "24                28               26               24               24\n",
      "25                29               28               25               25\n",
      "26                30               29               26               26\n",
      "27                31               30               27               27\n",
      "28                32               31               28               28\n",
      "29                33               32               29               29\n",
      "..               ...              ...              ...              ...\n",
      "584              707              632              584              584\n",
      "585              708              633              585              585\n",
      "586              709              634              586              586\n",
      "587              710              635              587              587\n",
      "588              711              636              588              588\n",
      "589              712              637              589              589\n",
      "590              714              638              590              590\n",
      "591              715              639              591              591\n",
      "592              716              640              592              592\n",
      "593              717              641              593              593\n",
      "594              718              642              594              594\n",
      "595              719              643              595              595\n",
      "596              720              644              596              596\n",
      "597              722              645              597              597\n",
      "598              723              647              598              598\n",
      "599              724              648              599              599\n",
      "600              725              649              600              600\n",
      "601              726              650              601              601\n",
      "602              727              651              602              602\n",
      "603              728              652              603              603\n",
      "604              732              653              604              604\n",
      "605              733              654              605              605\n",
      "606              735              657              606              606\n",
      "607              736              658              607              607\n",
      "608              737              659              608              608\n",
      "609              738              660              609              609\n",
      "610              740              661              610              610\n",
      "611              741              662              611              611\n",
      "612              743              663              612              612\n",
      "613              744              664              613              613\n",
      "\n",
      "[614 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "if not group_addresses_intra_list:\n",
    "    join_cols = ['Record_ID_list_1','Record_ID_list_2']\n",
    "    subset_columns_exact_matches = exact_matches[join_cols].copy()\n",
    "    subset_columns_exact_matches['row_index'] = subset_columns_exact_matches.index\n",
    "    subset_cols_golden_exact_matches = golden_exact_matches[join_cols].copy()\n",
    "    subset_cols_golden_exact_matches['row_index'] = subset_cols_golden_exact_matches.index\n",
    "    \n",
    "    subset_columns_exact_matches = subset_columns_exact_matches.astype(str)\n",
    "    subset_cols_golden_exact_matches = subset_cols_golden_exact_matches.astype(str)\n",
    "    \n",
    "    test_vs_golden_compare = mtch.exact_matcher(subset_columns_exact_matches, subset_cols_golden_exact_matches, join_cols)\n",
    "    print (test_vs_golden_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Record_ID_list_1, Record_ID_list_2, row_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if not group_addresses_intra_list:\n",
    "    missing_golden_matches = subset_cols_golden_exact_matches.mask(subset_cols_golden_exact_matches.row_index.isin(test_vs_golden_compare.row_index_list_2)).dropna()\n",
    "    print (missing_golden_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Record_ID_list_1, Record_ID_list_2, row_index]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if not group_addresses_intra_list:\n",
    "    matches_not_in_golden = subset_columns_exact_matches.mask(subset_columns_exact_matches.row_index.isin(test_vs_golden_compare.row_index_list_1)).dropna()\n",
    "    print (matches_not_in_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list 1 accuracy =  1.0\n",
      "list 2 accuracy =  1.0\n",
      "precision =  1.0\n",
      "recall =  1.0\n"
     ]
    }
   ],
   "source": [
    "if not group_addresses_intra_list:\n",
    "    total_records_list_1 = raw_address_list_1.shape[0]\n",
    "    total_records_list_2 = raw_address_list_2.shape[0]\n",
    "    total_modeled_matches = exact_matches.shape[0]\n",
    "    total_manual_exact_matches = golden_exact_matches.shape[0]\n",
    "    total_correct_positive_matches = test_vs_golden_compare.shape[0]\n",
    "    false_negatives = missing_golden_matches.shape[0]\n",
    "    false_positives = matches_not_in_golden.shape[0]\n",
    "    \n",
    "    accuracy_list_1 = (total_records_list_1 - (false_negatives + false_positives)) / total_records_list_1\n",
    "    accuracy_list_2 = (total_records_list_2 - (false_negatives + false_positives)) / total_records_list_2\n",
    "    precision = total_correct_positive_matches / (total_correct_positive_matches + false_positives)\n",
    "    recall = total_correct_positive_matches / (total_correct_positive_matches + false_negatives)\n",
    "    \n",
    "    print ('list 1 accuracy = ', accuracy_list_1)\n",
    "    print ('list 2 accuracy = ', accuracy_list_2)\n",
    "    print ('precision = ', precision)\n",
    "    print ('recall = ', recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
